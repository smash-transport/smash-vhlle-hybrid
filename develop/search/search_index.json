{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SMASH-vHLLE-Hybrid","text":"<p>Event-by-event hybrid model for the description of relativistic heavy-ion collisions in the low and high baryon-density regime. This model constitutes a chain of different submodules to appropriately describe each phase of the collision with its corresponding degrees of freedom. It consists of the following modules:</p> <p> SMASH hadronic transport approach to provide the initial conditions</p> vHLLE 3+1D viscous hydrodynamics approach to describe the evolution of the hot and dense fireball <p> \u00a0 CORNELIUS tool to construct a hypersurface of constant energy density from the hydrodynamical evolution (embedded in vHLLE)</p> <p> Sampler to perform Cooper-Frye particlization of the elements on the freezeout hypersurface</p> <p> SMASH hadronic transport approach to perform the afterburner evolution</p> <p>Give credit appropriately</p> <p>If you are using the SMASH-vHLLE-hybrid, please cite Eur.Phys.J.A 58(2022)11,230. You may also consult this reference for further details about the hybrid approach.</p>"},{"location":"developer/","title":"From great power comes great responsibility","text":"<p>The hybrid handler has been developed trying to make the entry point of a newbie developer as low as possible. Although  Bash can be sometimes not that simple to read<sup>1</sup>, the integration-operation segregation principle (IOSP) has been used most of the times at the the top-level to allow the reader to get a first understanding of what the main functions are doing, having then the possibility to deepen into lower levels if needed. Said differently, reading the code top-down should be straightforward, as the most top-level function is a series of function calls only, whose names should clearly describe what is done.</p> <p>Therefore, this guide is not meant to document any possible implementation detail, but rather provide the reader with important information, which might be difficult to grasp by reading the codebase. Assumptions that are crucial to know before even getting to the code have a dedicated card here in the following.</p> <p>No period or space in YAML keys</p> <p>The full implementation relies on the assumption that YAML keys contain neither spaces nor periods. This is basically a design decision for this project and also a sensible thing to avoid (it would require another level of quoting when using <code>yq</code>). The developer has to be aware that the code does break, if such an assumption is violated. Although the external software used for each stage might violate this, it is not the case at the moment and it is considered unlikely to happen in the future.</p> <p>The stages names are hard-coded everywhere</p> <p>Each simulation phase has a label associated to it (i.e. <code>IC</code>, <code>Hydro</code>, etc.) and these are used in variable names as well, although with lower-case letters only. In the codebase it has been exerted leverage on this aspect and at some point the name of variables are built using the section labels transformed into lower-case words. Hence, it is important that section labels do not contain characters that would break this mechanism, like dashes or spaces.</p> <p>Ensure variables that should be defined are defined</p> <p>In Bash, from within a function, it is possible to access the caller local variables. However, this should be used with care and not be abused as it might lead to code which is easy to use wrong. When needed, make sure to use the dedicated utility functions <code>Ensure_That_Given_Variables_Are_Set</code> and <code>Ensure_That_Given_Variables_Are_Set_And_Not_Empty</code> in the beginning of your function. This is also a way to document the implementation.</p> <ul> <li> <p> Ready to code?</p> <p>Cool, but not so quick. Every project has its contributing rules and you are kindly requested to go over them at least once before starting.</p> <p>\u00a0 Read more</p> </li> <li> <p> Building the documentation</p> <p>Especially when changing it, it is important to locally build the documentation and see how it looks. This is made trivial by MkDocs which also helps deploying it at every release.</p> <p>\u00a0 Check it out!</p> </li> <li> <p> A handy testing framework</p> <p>Tests are the key to be confident that the code works as expected. A testing framework has been developed tailored to the project. Adding new tests is as simple as writing a new function!</p> <p>\u00a0 Check it out!</p> </li> <li> <p> New release procedure</p> <p>At every new release few standard steps are required and it is important not to forget them and to always be consistent in how a new version of the code is released.</p> <p>\u00a0 Discover more!</p> </li> <li> <p> Utility functions</p> <p>As in every codebase, there are operations that are totally general and can be delegated to utility functions that will help in extending or improving the codebase.</p> <p>\u00a0 Reference page</p> </li> <li> <p> Design of the parameter scan</p> <p>Since the <code>prepare-scan</code> execution mode implementation is not trivial and it required a couple of design decisions, it has been decided to comment on each of these in a dedicated page.</p> <p>\u00a0 Read more</p> </li> <li> <p> Adding a new module</p> <p>You want to replace the software in one or more stages with a different one? Check these notes about what changes are needed.</p> <p>\u00a0 Read more</p> </li> </ul> <ol> <li> <p>Alessandro has offered many times a Bash crash-course and the full material is available on his GitHub profile   AxelKrypton.\u00a0\u21a9</p> </li> </ol>"},{"location":"developer/building_docs/","title":"Building the documentation","text":"<p>The documentation is built using MkDocs and in particular the impressive Material for MkDocs theme. All documentation files and related material is located in  docs. Therefore it is hosted in the codebase and it naturally evolves together with it.</p>"},{"location":"developer/building_docs/#prerequisite","title":"Prerequisite","text":"<p>Few packages are required and all can be installed using <code>pip</code>: <pre><code>pip install mkdocs\npip install mkdocs-material\npip install mike\n</code></pre></p> <p>You can refer to the corresponding installation pages for further information (1).</p> <ol> <li> MkDocs Material for MkDocs mike</li> </ol>"},{"location":"developer/building_docs/#serving-building-and-deploying","title":"Serving, building and deploying","text":"<p>The deployed version is hosted on the  <code>gh-pages</code> branch, while documentation from any other branch can be comfortably visualized in a browser opening up http://127.0.0.1:8000/ after having run <code>mkdocs serve</code> in a terminal from the top-level repository folder.</p> <p>The documentation website can also be locally built by running <code>mkdocs build</code>, which will create a  site folder containing the website source code.</p> <p>Once changes to the website have been locally checked, they are ready to be deployed. We use <code>mike</code> to deploy documentation, because it allows to make different documentation versions coexist and be easily selected. Our usage of this tool is quite basic, but you can check out mike's documentation to know more about it.</p> <p>Be aware of few caveats</p> <p>Even if we use <code>mike</code> to deploy documentation, the same warnings about the MkDocs deployment feature apply:</p> <ol> <li>Be aware that you will not be able to review the built site before it is pushed to GitHub. Therefore, you may want to verify any changes you make to the docs beforehand by using the build or serve commands and reviewing the built files locally.</li> <li>You should never edit files in your pages repository by hand if you're using the gh-deploy command because you will lose your work the next time you run the command.</li> <li>If there are untracked files or uncommitted work in the local repository where <code>mkdocs gh-deploy</code> is run, these will be included in the pages that are deployed.</li> </ol>"},{"location":"developer/building_docs/#our-deployment-scheme","title":"Our deployment scheme","text":"<p>The documentation does not store one version for all versions of the codebase. In particular, releases like a bug-fix which only change the <code>patch</code> number in the version string won't have a new documentation version associated. To say it differently, the documentation of version <code>X.Y</code> will always show the documentation of the corresponding highest patch.</p>"},{"location":"developer/building_docs/#at-new-releases","title":"At new releases","text":"<p>When a new release of the codebase is done, a new version of the documentation should be built and deployed. This can be easily done via <pre><code>mike deploy --push --update-aliases X.Y latest\n</code></pre> where <code>X.Y</code> are the <code>major.minor</code> version numbers of the release. The command will also update the <code>latest</code> alias to point to the new release documentation.</p> Good to know <p>Omitting the <code>--push</code> option, nothing will be pushed and you have the possibility to check the changes on the <code>gh-pages</code> branch, which must be then manually pushed to publish the changes.</p>"},{"location":"developer/building_docs/#at-new-patches","title":"At new patches","text":"<p>When a new patch is released, it is usually worth adjusting the documentation title via <code>mike retitle --push X.Y &lt;new_title&gt;</code>, where new title might simply be <code>X.Y.Z</code>, i.e. the complete new version string. This however does not rebuild the documentation and, therefore, it is needed to run a deploy command, too. Two alternatives are possible:</p> <ol> <li>First the already deployed version is retitled and then the documentation updated    <pre><code>$ mike retitle --push X.Y &lt;new_title&gt;\n$ mike deploy --push X.Y\n</code></pre></li> <li>or the documentation is updated first and then retitled.    <pre><code>$ mike deploy --push --update-aliases X.Y latest\n$ mike retitle --push X.Y &lt;new_title&gt;\n</code></pre></li> </ol> <p>Do not deploy using the patch version as identifier!</p> <p>If you use the new title like <code>X.Y.Z</code> as identifier to deploy, a new documentation version will be created and the old one not simply updated. This is not desired!</p>"},{"location":"developer/building_docs/#the-development-documentation","title":"The development documentation","text":"<p>Documentation between releases will naturally evolve and, whenever it makes sense, changes to the documentation should be deployed, too. For this purpose, developers should run <pre><code>mike deploy --push develop\n</code></pre> reasonably often, since this documentation is thought to reflect the state of the  <code>develop</code> branch.</p>"},{"location":"developer/contributing/","title":"Contributing to SMASH-vHLLE-Hybrid","text":"<p>If you landed here, you might be thinking of contributing to the codebase: Excellent decision! </p> <p>As an external contributor, fork the repository, work on a branch dedicated to your changes and then create a pull request. Details on this workflow can be found e.g. here.</p> <p>In any case, before starting editing code, take few minutes to go through the following recommendations. It will speed up the code review and avoid comments about codebase notation.</p> <p>Write tests first</p> <p>The project has been started with the goal of making every developer fearless in changing and improve the code. Such a confidence can only be attained if the developer can be confident that their changes are not breaking existing functionality. Sounds like magic? No, it simply means that there are tests for basically every functionality.  You are requested to stick to this existing aspect and, possibly, device and why not write tests for your new code before implementing the code itself. A sufficiently handy testing framework has been developed tailored to this project and you find the needed information about it in a dedicated page  testing framework. The basic idea is that adding new tests is as simple as adding a new function.</p> <p>It's a Bash project!</p> <p>Use the <code>.bash</code> extension (NOT <code>.sh</code>) for files containing Bash code. This is often ignored in shell scripting.</p> <p>Keep an eye on system requirements</p> <p>Whenever you need to use an external command, you should ask yourself whether this is a new system requirement and, if so, whether it is really needed. Often in Bash world you can achieve the same task in different ways and it might be that you do not need to add a further dependency. Sometimes it is really needed and, in this case, you must not forget to add the requirement to the system requirements checks<sup>1</sup>. The same applies to python requirements, which not only need to be added to the  python/requirements.txt file. Unless they are always required, you need to</p> <ol> <li>add a comment about under which condition the new requirements are needed to the Bash code where system requirements are checked (this will be displayed to the user);</li> <li>implement the condition to limit the check to the runs which need such requirements (this has been done where the sanity checks are performed);</li> <li>ensure tests pass (they might require setting some more global variables in the <code>tests_runner</code>, depending on the new conditions).</li> </ol>"},{"location":"developer/contributing/#editing-existing-files-or-creating-new-ones","title":"Editing existing files or creating new ones","text":"<p>This codebase is distributed under the terms of the GPLv3 license. In general, you should follow the instructions therein. Some guidance for authors is also provided here, in order to reach a coherent style of copyright and licensing notices.</p> <p>Getting started working on the project</p> <ul> <li>If you are contributing for the first time, be sure that your git username and email are set up correctly. To check them you can run <code>git config --list</code> and see the values of the <code>user.name</code> and <code>user.email</code> fields. Add yourself to the AUTHORS file (you find this at the top-level of the repository).</li> <li>If you create a new file, add copyright and license remarks to a comment at the top of the file (after a possible shebang). This should read like <pre><code>#===================================================\n#\n#    Copyright (c) 2024\n#      SMASH Hybrid Team\n#\n#    GNU General Public License (GPLv3 or later)\n#\n#===================================================\n</code></pre> with the correct development year.</li> <li>When editing an existing file, ensure there is the current year in the copyright line. The years should form a comma-separated list. If two subsequent years or more are given, the list can be merged into a range.</li> </ul>"},{"location":"developer/contributing/#hooks-and-good-commits","title":"Hooks and good commits","text":"<p>In order to e.g. avoid to forget to update the copyright statement, git hooks can be used to perform some checks at commit time. You can implement them in your favorite language or look for some on the web. We encourage you to checkout and use the  GitHooks which will also enforce good habits about how to structure your commit messages, as well as sanitize white spaces in source code.  Please, refer to their README file for more information.</p>"},{"location":"developer/contributing/#used-bash-behavior","title":"Used Bash behavior","text":"<p>After long consideration, it has been decided to use some stricter Bash mode. In particular, the harmless <code>pipefail</code>, <code>nounset</code> and <code>extglob</code> options are enabled, together with the more controversial <code>errexit</code> one (and its sibling <code>inherit_errexit</code>). We are aware that the <code>errexit</code> option leads to many corner cases and that there are controversial opinions around. The advantage is its useful semantics and, more importantly, it can protect from dangerous situations (<code>cd NotExistingFolder; rm -r *</code>). But beware of possible gotchas. Of course, a proper error handling would be even better. Considering that even more inexperienced developers might contribute to the project and miss some error handling (and, by the way, oversight is always possible), we still decided to let the shell abort on error, accepting to deal with all possible downsides this feature has.</p> <p>Finally, a short remark about <code>extglob</code> option. To motivate why we decided to enable it globally, it is best to quote Greg's wiki:</p> <p><code>extglob</code> changes the way certain characters are parsed. It is necessary to have a newline (not just a semicolon) between <code>shopt -s extglob</code> and any subsequent commands to use it. You cannot enable extended globs inside a group command that uses them, because the entire block is parsed before the <code>shopt</code> is evaluated. Note that the typical function body is a group command. An unpleasant workaround could be to use a subshell command list as the function body.</p>"},{"location":"developer/contributing/#bash-notation-in-the-codebase","title":"Bash notation in the codebase","text":"<p>The general advice is pretty trivial: Be consistent with what you find.</p> <p>The codebase is formatted using <code>shfmt</code>. Any developer should be aware that, because of the nature of the Bash scripting language, it is probably impossible to have a perfect formatter, which ensure rules in all details (as e.g. <code>clang-format</code> does for C++). Therefore, it is crucial for the developer to stay consistent with the existing style and, more importantly, to take some minutes to read the following lists. In particular, be aware the the formatter will not enforce the rules explained below.</p> <p>The main hybrid handler script can format the codebase!</p> <p>Before opening a PR, make sure all tests pass. One of them will try to check formatting and complain if something has to be adjusted. The main script has a <code>format</code> execution mode which formats the full codebase and runs the formatting unit test. Use it by running <code>./Hybrid-handler format</code>. This is meant for developers only and therefore does not appear in the helper description.</p> <p>Some aspects about the codebase</p> <ul> <li>Lines of code are split around 100 characters and should never be longer than 120.   This is a hard limit and tests will fail if longer lines exist.</li> <li>Bash functions use both the <code>function</code> keyword and parenthesis (with the enclosing braces on separate lines). <pre><code>function Example_Function()\n{\n    # Body of the function\n}\n</code></pre></li> <li>Local variables are typed with all small letters and words separated by underscores, e.g. <code>local_variable_name</code>.</li> <li>Global variables in the codebase are prefixed by <code>HYBRID_</code> and this is meant for better readability, e.g. <code>HYBRID_global_variable</code>.</li> <li>Analogously, global variables in tests are prefixed by <code>HYBRIDT_</code>.</li> <li>Variables are always expanded using braces, i.e. you should use <code>${variable}</code> instead of <code>$variable</code>.</li> <li>Function names are made of underscore-separated words with initials capitalized, e.g. <code>Function_Name_With_Words</code>.</li> <li>Functions that are and should be used only in the file where declared are prefixed by <code>__static__</code>.</li> <li>Quotes are correctly used, i.e. everything that might break if unquoted is quoted.</li> <li>Single quotes are used if there is no need of using double or different quotes.</li> <li>All functions declared in each separate file are marked in the end of the file as <code>readonly</code>.   This is done calling the <code>Make_Functions_Defined_In_This_File_Readonly</code> function.   This does not apply to the code in the  tests folder.</li> <li>Files are sourced all together by sourcing a single dedicated file (cf.  bash/source_codebase_files.bash file).</li> </ul> <p>Other conventions in use that the formatter enforces</p> <ul> <li>Indentation is done exclusively with spaces and no Tab should be used.</li> <li>Loops and conditional clauses are started on a single line, i.e. the <code>do</code> and <code>then</code> keywords are NOT put on a separate line.</li> </ul> <ol> <li> <p>For OS requirements, it should be simply enough to add the new requirements to the appropriate array in the function where system requirements are defined.\u00a0\u21a9</p> </li> </ol>"},{"location":"developer/new_module/","title":"Adding a new module","text":"<p>Although the SMASH-vHLLE-hybrid, as implied in the name, was built with a defined set of software modules, it is designed to be easily extensible with new modules. This allows, amongst other things, to test new algorithms and study model dependency.</p> <p>Take inspiration from the existing modules</p> <p>The first alternative module to be implemented was the FIST sampler, which shares many similarities with the SMASH sampler. The implementation of the FIST sampler can be used as a reference for the implementation of new modules.</p> <p>Take the following instructions with a pinch of salt</p> <p>Giving coding instructions in documentation has always the drawback that these might get outdated or simply become wrong with the evolution of the codebase. In this case the mentioned snippets of code are unlikely to be invalid at some point, but you should keep in mind that something might have to be done in a slightly different way. If so, you can take the opportunity to refine these instructions. </p> <p>Adding a new module to the SMASH-vHLLE-hybrid is a straightforward process. The following steps serve as a guidance and will probably be needed. Let's assume we want to add MUSIC as an alternative for the hydrodynamic stage:</p> <ul> <li> <p>In case the stage has no other modules, add a field to the <code>HYBRID_module</code> array in the  global_variables.bash script for the respective stage .</p> <p>global_variables.bash<pre><code>declare -gA HYBRID_module=(\n    [Sampler]=\"${HYBRID_default_sampler_module}\"\n    [Hydro]=\"${HYBRID_default_hydro_module}\"\n)\n</code></pre>   where the <code>HYBRID_default_hydro_module</code> has to be defined above in the same file.</p> </li> <li> <p>At the same place, add alternative base configuration files for the stages named <code>&lt;STAGE_NAME&gt;_&lt;MODULE_NAME&gt;</code>  and add   <code>[Module]</code> to the valid keys of the stage.</p> global_variables.bash<pre><code>declare -rgA HYBRID_hydro_valid_keys=(\n    [Executable]='HYBRID_software_executable[Hydro]'\n    [Config_file]='HYBRID_software_base_config_file[Hydro]'\n    [Input_file]='HYBRID_software_user_custom_input_file[Hydro]'\n    [Module]='HYBRID_module[Hydro]'\n    [Scan_parameters]='HYBRID_scan_parameters[Hydro]'\n    [Software_keys]='HYBRID_software_new_input_keys[Hydro]'\n)\ndeclare -gA HYBRID_software_base_config_file=(\n    [IC]=\"${HYBRID_default_configurations_folder}/smash_initial_conditions.yaml\"\n    [Hydro]=\"\"\n    [Sampler]=\"\"\n    [Afterburner]=\"${HYBRID_default_configurations_folder}/smash_afterburner.yaml\"\n    [Sampler_SMASH]=\"${HYBRID_default_configurations_folder}/hadron_sampler\"\n    [Sampler_FIST]=\"${HYBRID_default_configurations_folder}/fist_config\"\n    [Hydro_vHLLE]=\"${HYBRID_default_configurations_folder}/vhlle_hydro\"\n    [Hydro_MUSIC]=\"${HYBRID_default_configurations_folder}/music_hydro\"\n)\n</code></pre> Why are <code>[Hydro]</code> and <code>[Sampler]</code> set to empty strings? <p>In this function, called very early in the main script, only global variables are declared and not all the information is available. In particular, it is not yet known which software will be chosen by the user (e.g. vHLLE or MUSIC). Such a variable will be set after having parsed the handler configuration file and it will be used in the actual implementation of the stage-specific operations.</p> </li> <li> <p>Add a base configuration in the  configs folder for the new module.</p> </li> <li> <p>Adapt the check for valid modules in the  sanity_checks.bash script by modifying the <code>if</code>-clause as below and adding a new version of each function for the new stage.</p> sanity_checks.bash<pre><code>if [[ \"${key}\" =~ ^(Hydro|Sampler)$ ]]; then\n    __static__Ensure_Valid_Module_Given_For_${key}\n    __static__Choose_Base_Configuration_File_For_${key}\n    __static__Ensure_Additional_Paths_Given_For_${key}\n    __static__Set_${key}_Input_Key_Paths\nfi\n</code></pre> </li> <li> <p>Add versions of the affected stage functionality script for the new module.   Shared logic stays in the common functionality script, whereas module-specific logic is implemented in the module-specific script.   The module dependent logic is implemented in functions with the suffix <code>_${MODULE_NAME}</code> .   Therefore, <code>if</code>-clauses in the main stage script can be avoided.</p> </li> <li> <p>Add new behavior to the black-box script for the new module in the  tests/mocks folder.   The switch should be possible to be done via an environment variable as done for the hadron sampler.</p> </li> <li> <p>Don't forget to source the new files! </p> </li> </ul>"},{"location":"developer/parameters_scan/","title":"Design of parameter scans","text":"<p>Reading the code is always encouraged</p> <p>Reading the code starting from the main script and following the function calls will probably already give a good overview of how the <code>parameter-scan</code> mode is implemented. However, a couple of comments might help understanding the workflow and the rationale behind it.</p> <pre><code>graph LR\n  subgraph one[\"Parsing and validation\"]\n    direction TB\n    A(\"Parse scan parameters\") --&gt; B(\"Validate and store scan YAML map\")\n    B --&gt; C(\"Create list of single parameters values\")\n  end\n  subgraph two[\"Create and populate scan folder\"]\n    direction TB\n    D(\"Get parameters combinations\") --&gt; E(\"Prepare scan folder and combination file\")\n    E --&gt; F(\"Append parameter combination to file\")\n    F --&gt; G(\"Create output file\")\n  end\n  one --&gt; two\n\n  style one fill:none,stroke:#ff7f50;\n  style two fill:none,stroke:#ff7f50;</code></pre> <p>Premature optimization is the root of all evil. - Donald Knuth</p> <p>The present implementation of the <code>prepare-scan</code> mode might appear not that clever, since many intermediate parsing and storing lead to e.g. iterating over the same arrays many times. On the same line, probably some <code>yq</code> invocation might be spared by combining more operation in the same call. However, this has been done on purpose trying to always prefer clearer code over claimed speed in the very first implementation. Out of the box, generating a scan with few hundreds parameters combinations takes few seconds on an average laptop and it is reasonable to believe that this is an acceptable performance.</p>"},{"location":"developer/parameters_scan/#lack-of-multi-dimensional-arrays","title":"Lack of multi-dimensional arrays","text":"<p>In Bash it is not possible to have an array as entry of another array. However, for the parameter scan, it is needed to somehow store a list of parameters, each with a list of possible values and this would be a natural usage of a two-dimensional array.</p> <p>By design, scan parameters must have either boolean or numerical value and this also means that they do not contain spaces. Hence, it is possible to store a list of values as a single string and then use word splitting to e.g. create an array out of it with values stored as separate entries.</p> Proof of concept<pre><code>$ list=\"1 2 3\"\n$ array=( ${list} ) # This expansion must be unquoted to let word splitting act\n$ printf '%s\\n' \"${array[@]}\"\n1\n2\n3\n</code></pre> <p>The same trick is used to store the list of scan parameters. This is possible because the YAML keys happen not to have spaces in them.</p> <p>Spaces in YAML key or values would break the implementation</p> <p>Since we rely on word splitting to correctly split values or parameter names, this would be automatically broken if spaces were part of values, since word splitting would then split a single value in different ones. If having spaces in keys or values turns out to be a need, some more involved implementation is required.</p>"},{"location":"developer/parameters_scan/#interplay-between-bash-and-yaml","title":"Interplay between Bash and YAML","text":"<p>The scan information in the configuration file must be parsed into Bash variables and the first aspect to deal with is the fact that the user is free to use YAML syntax in its full glory. For example, sequences can be specified using square brackets or an explicit list. We do not want to deal this ambiguity on the Bash side and therefore we make sure that everything is stored in the same way. The <code>yq</code> tool offers a way to print out sequences in a compact form.</p> Enforcing compact style reading out YAML sequences<pre><code>$ echo $'- 17\\n- 42\\n- 666'\n- 17\n- 42\n- 666\n$ yq '.. style=\"flow\"' &lt;&lt;&lt; $'- 17\\n- 42\\n- 666'\n[17, 42, 666]\n</code></pre> <p>This is used both when parsing the <code>Scan_parameters</code> key and when parsing each parameter values.</p>"},{"location":"developer/parameters_scan/#storing-scan-parameters-values","title":"Storing scan parameters values","text":"<p>Before having a list of values for each scan parameter, its scan YAML map has to be extracted from the handler configuration file and then parsed to generate the list of values. This might be done in a single swipe but would make it basically impossible to later add sampling algorithms that need to consider all scan parameters at the same time to produce their list of values (e.g. Latin Hypercube Sampling). Therefore the scan YAML maps are first stored and then used at a later point to produce the list of values of each parameter.</p> <p>A single Bash associative array has been chosen for this purpose. Its keys are the parameters stored as a period-separated list of YAML keys as they appear in the hybrid handler configuration file, and precisely in the 'Software_keys' sections. The value of each array entry is, in a first phase, the YAML scan map. Later, this is replaced by a string containing a list of parameter values. In this case a YAML sequence style is kept, i.e. with squares and commas.</p> Final contentFirst phase Example of parameter values array<pre><code>list_of_parameters_values=(\n    ['IC.Software_keys.Modi.Collider.Sqrtsnn']='[4.3, 7.7]'\n    ['Hydro.Software_keys.etaS']='[0.13, 0.15, 0.17]'\n)\n</code></pre> Example of parameter values array<pre><code>list_of_parameters_values=(\n    ['IC.Software_keys.Modi.Collider.Sqrtsnn']='{Values: [4.3, 7.7]}'\n    ['Hydro.Software_keys.etaS']='{Values: [0.13, 0.15, 0.17]}'\n)\n</code></pre>"},{"location":"developer/parameters_scan/#generating-all-possible-combinations","title":"Generating all possible combinations","text":"<p>Technically speaking, this is straightforward in Bash, thanks to the built-in brace expansion.</p> Example of brace expansionOrder of expansionWorkaround using eval <pre><code>$ printf '%s\\n' {a,b,c}_{1,2}\na_1\na_2\nb_1\nb_2\nc_1\nc_2\n</code></pre> <pre><code>$ var=\"{a,b,c}_{1,2}\"\n$ printf '%s\\n' ${var}\n{a,b,c}_{1,2}\n</code></pre> <pre><code>$ var=\"{a,b,c}_{1,2}\"\n$ eval printf '%s\\\\\\n' \"${var}\"\na_1\na_2\nb_1\nb_2\nc_1\nc_2\n</code></pre> <p>However, the Bash shell expansion mechanism works in a way such that brace expansion happens before variables expansion. To make brace expansion act on a string that is contained in a variable requires using <code>eval</code>, which is always delicate, because it might open up to security flaws like enabling code injections. In this case the strings passed to it are validated and guaranteed to be numbers only, though.</p> <p>Allowing string values requires action here!</p> <p>If at some point it is needed to support scans on parameter of string type, a different approach should be probably used to generate all combinations. Probably, as it is, the code would also break in case of spaces into the strings.</p>"},{"location":"developer/release_procedure/","title":"Releasing procedure","text":"<p>The codebase development is done using the Git-flow branching pattern<sup>1</sup>.</p> <p>Use a Git extension</p> <p>There are few extensions that allow to automatize the overhead of sticking to the branching model by hand, i.e. by using Git vanilla commands. Since trunk based development has become in the software landscape more appealing, most Git extension to implement Git-flow pattern have decreased maintenance and many froze. However, in simple scenarios, like manual usage in the terminal, any extension should work as expected. You can check-out the AVH edition (which has been archived in June 2023) or the CJS Edition which is still (rarely) active. Both are successors of the original implementation by Vincent Driessen, who invented the model in first place.</p> <p>When the code is ready to be published, a <code>release</code> branch shall be created and the finalization steps should be carried out on it. Closing the branch means to merge it into <code>main</code>, which will be tagged and contain the released version, and merge it back to <code>develop</code>, which in principle might have continue development.</p> <p>Don't forget to update the version string</p> <p>In the codebase there is a global <code>HYBRID_codebase_version</code> variable, which contains the version label. This should be bumped on the <code>release</code> branch, i.e. when it is clear which will be the following version number. Analogously, it should be bumped into a dirt state<sup>2</sup> on <code>develop</code> as soon as the <code>release</code> is closed.</p>"},{"location":"developer/release_procedure/#release-checklist","title":"Release checklist","text":"<ul> <li> <p> Create the <code>release</code> branch from <code>develop</code> and switch to it.</p> If you are doing a hot-fix, an extra step is needed here! <p>For hot-fixes, you will branch off <code>main</code> and the repository will be in a stable state. Hence, you need to immediately bump the version to the hot-fix one adding e.g. a <code>-start</code> suffix. Before closing the branch you will remove such a suffix.</p> </li> <li> <p> Make sure everything is ready to go (e.g. check copyright statements including in documentation).</p> </li> <li> Change the <code>Unreleased</code> box in the CHANGELOG file to a new release section by<ul> <li>adding the new section title;</li> <li>changing the type of box;</li> <li>adding release date and link to changes from previous version.</li> </ul> </li> <li> Bump version number global variable in main script to a stable state.</li> <li> Close the <code>release</code> branch in the git-flow sense:<ul> <li>merge it into the <code>main</code> branch;</li> <li>switch to <code>main</code> and tag the last commit;</li> <li>switch to <code>develop</code> and merge the <code>release</code> back into it<sup>3</sup>.</li> </ul> </li> <li> Publish the new release by pushing the changes and the new tag on <code>main</code>.</li> <li> From <code>main</code> build and deploy the documentation.</li> <li> From <code>develop</code> bump version number global variable in main script to an unstable state and prepare a <code>!!! work-in-progress \"Unreleased\"</code> box in the CHANGELOG file.</li> </ul> Create the release branchClose the release branchPublish new release <pre><code># Git-flow extension\ngit flow release start 1.2.0\n\n# Vanilla Git commands\ngit switch -c release/1.2.0 develop\n</code></pre> <pre><code># Git-flow extension\ngit flow release finish 1.2.0\n\n# Vanilla Git commands\ngit switch main\ngit merge --no-ff release/1.2.0\ngit tag -a 1.2.0\ngit switch develop\ngit merge --no-ff release/1.2.0\ngit branch -d release/1.2.0\n</code></pre> <pre><code># Only vanilla Git commands\ngit push origin main\ngit push origin develop\ngit push origin --tags\ngit push origin :release/1.2.0  # if pushed\n</code></pre> <ol> <li> <p>Alessandro has offered a trilogy of talks about Git and these are available on his GitHub profile   AxelKrypton. The second part of the last talk is devoted to introduce and explain this branching pattern in detail.\u00a0\u21a9</p> </li> <li> <p>The version variable should make clear whether the codebase is in a stable state or not. Usually a suffix like <code>-next</code> or <code>-unreleased</code> is added to the last stable version name immediately after the release (in a dedicated commit), such that the main script will report the dirty state when run with the <code>--version</code> option.\u00a0\u21a9</p> </li> <li> <p>This might lead to conflicts if the codebase has evolved on the <code>develop</code> branch. Please note as well that merging <code>main</code> into <code>develop</code> would be equivalent as the <code>main</code> branch is meant for stable releases only and it should not contain anything new w.r.t. the <code>develop</code> branch.\u00a0\u21a9</p> </li> </ol>"},{"location":"developer/testing_framework/","title":"A hand-crafted testing framework","text":""},{"location":"developer/testing_framework/#a-word-of-caution","title":"A word of caution","text":"<p>Before describing how the framework works and how to run tests, it is worth spending a few words about a Bash technicality, which could bite you at some point and should not be underestimated.</p> <p>The Bash behavior of the tests runner</p> <p>The shell <code>errexit</code> option is not enabled in the tests runner and, therefore, its Bash behavior when running tests is not the same as when running the handler. This is a natural choice in a testing framework where the error handling is wished to be done by hand to e.g. count failures and build up a report about failed tests. However, the <code>errexit</code> mode is enabled when running the codebase functions, otherwise the real behavior of the handler code would not be tested!</p> <p>DANGER: Be aware of contexts that make the shell ignore <code>errexit</code> behavior</p> <p>Although the note above might totally make sense to you, it is important to stress that in tests you really want to run the codebase functions with <code>errexit</code> enabled, since the real runs rely on it. You might be wondering: Sure, what's the danger then? In tests functions, a <code>return 1</code> statement makes the test fail (see further information below on this page). Refactoring code e.g. to reduce duplication, you might want to delegate a task to another function, which runs the handler code under test and for instance returns 1 in case of failure. So far so good. However, you would also like to make the test fail if the called function fails. You might then innocently add a trailing <code>|| return 1</code> to the function call.  Gotcha! What did you just do? You made the called function code, and hence the handler code, run in a context were <code>errexit</code> is ignored (i.e. on the left of a <code>||</code> operator).  Keep this in mind! If you really want to make the test fail, you have to do so in a different way, for example passing the exit code of your delegated task to a dedicated new function which can then harmlessly support a trailing <code>|| return 1</code>.</p>"},{"location":"developer/testing_framework/#running-tests","title":"Running tests","text":"<p>In the  tests folder there is a  <code>tests_runner</code> executable, which shall be used to run tests. This can be run with the <code>--help</code> command line option to obtain a self-explanatory helper message.</p> <p>Using the test runner it is possible to either run unit or functional tests. This is achieved via a first positional argument. By default all available running tests are run and a detailed report is printed to the standard output. However, it is possible to only get a list of tests, run some of them, decrease the verbosity of the report and/or explicitly ask to keep the produced output files.</p> Running all testsRunning some testsKeeping output files <pre><code># Run unit tests\n./tests_runner unit\n# Run functional tests\n./tests_runner functional\n</code></pre> <pre><code># Get list of available unit tests\n./tests_runner unit -t\n# Run tests 3 to 7 and 10\n./tests_runner unit -t '3-7,10'\n</code></pre> <pre><code>./tests_runner unit -k\n</code></pre> <p>Tests output files are created in the  tests/run_tests folder and each test creates an own sub-folder. If all tests pass, the output folder is automatically deleted, unless differently asked. However, if at least one test fails, the output folder is kept to allow to investigate the failure. The  tests_runner.log can be found inside it and this is a good starting point to check out what happened.</p> <p>A handy command for development</p> <p>From time to time it will happen that a test has to be run many times and the runner log file inspected, for example because something does not work as expected. It is helpful to check out the test number and run it in the following way (from the  tests folder), <pre><code>./tests_runner unit -k -t 42 || less -rf run_tests/tests_runner.log\n</code></pre> where <code>||</code> can be substituted by <code>;</code> if it is desired to always have a look to the log file, i.e. also if the test passes.</p> <p>All output folders are kept by default</p> <p>When asking to keep the tests output folder, it might happen that a  tests/run_tests already exists (because of a previous run). The runner will automatically create a new one, renaming the older one with a timestamp prefix referred to when the renaming has taken place. If tests are run many times always keeping the output folders, for example during development, at some point these might be too many. It is totally safe to delete them. You can do it with a simple command like <code>rm -r run_tests*</code> from the  tests folder.</p>"},{"location":"developer/testing_framework/#how-can-i-use-the-test-framework-and-what-should-i-know","title":"How can I use the test framework and what should I know?","text":"<p>In order to be allow the runner take over most of the overhead and be as user-friendly as possible, some discipline from the developer is requested and the following few mechanisms should be understood and respected.</p> <p>Filenames must have a given prefix!</p> <p>Since the runner automatically looks for tests, these have to be put in filenames with a given prefix and with the <code>.bash</code> extension.</p> <ul> <li>Unit tests are recognized as such only if put in files matching <code>unit_tests_*.bash</code> globbing expression.</li> <li>Functional tests are recognized as such only if put in files matching <code>functional_tests_*.bash</code>.</li> <li>Tests put in other files are totally ignored by the runner.</li> </ul> <p>In general any test has a setup-run-teardown flow, which is automatically enforced by the runner.</p> <pre><code>graph TB\n    classDef my fill:#2b9b461a,stroke:#2b9b46;\n    A(\"Make test preliminary operations\") --&gt; B(\"Run test\")\n    B --&gt; C(\"Clean up operations\")\n    class A,B,C my;</code></pre> <p>The developer can decide what should be done in each phase and the runner will implicitly understand whether anything has to be done in each phase simply from the presence or absence of Bash functions with a given name. The existence of a test and its name is automatically deduced from the function to run it, which has to follow a given pattern (see examples below). If such a function exists, the runner will check for existence of the corresponding setup and teardown functions, invoking them before and after the test, respectively, if existing.</p> Setup functionUnit test functionFunctional test functionTeardown function <pre><code>function Make_Test_Preliminary_Operations__test-name()\n{\n    # Test preliminary operations\n}\n</code></pre> <pre><code>function Unit_Test__test-name()\n{\n    # Unit test code\n}\n</code></pre> <pre><code>function Functional_Test__test-name()\n{\n    # Functional test code\n}\n</code></pre> <pre><code>function Clean_Tests_Environment_For_Following_Test__test-name()\n{\n    # Test clean up operations\n}\n</code></pre> <p>Use the framework functionality to invoke code to test</p> <ul> <li>Codebase functions to be invoked in unit tests should be called through the <code>Call_Codebase_Function</code> and <code>Call_Codebase_Function_In_Subshell</code> interface functions (passing the name of the function to be invoked as first argument and the arguments to be forward afterwards).</li> <li>In functional tests you'll probably want to run the hybrid handler with some options and this can be easily achieved by using the <code>Run_Hybrid_Handler_With_Given_Options_In_Subshell</code> function.</li> </ul> <p>Each test is run in its own subshell</p> <p>The full test flow, including setup and teardown, is run in a subshell in order to isolate its changes from the external environment. In this way tests should not interfere with each other and the developer should be aware of this aspect. For example, if the same setup operations are needed in different tests, they should be repeated for every test, but this can be easily achieved e.g. by letting a setup function invoke that of a different test.</p> <p>No space must occur before the <code>function</code> keyword</p> <p>Although enforced by the formatter, it might happen that some spaces are put bofere the <code>function</code> Bash keyword in the function defining a test. If this is done, the test will not be recognized by the runner.</p>"},{"location":"developer/utility_functions/","title":"Bash utility functions","text":"<p> Bash world is often the realm of the do it yourself.  This is belonging to the nature of the language as it is very unusual to have libraries with functionality to be reused. However, it is very simple to provide the codebase with some utility function tailored to the needs of the project. In the following you will find an overview of what is available in the codebase.</p> <p>These functions are tailored to the project</p> <p>The following functions should not be considered as a library. Actually most of them would not work if copied and pasted in another project. Be aware that some of them depend on project-specific aspects (e.g. they use the logger or they assume that the shell used options are on) and, sometimes, make use of other utility functions.</p> <p>Mimicking boolean functions</p> <p>In Bash conditionals are based on exit codes of commands and therefore a function which returns either 0 (success) or 1 (failure) can directly be used in a conditional clause, e.g. a <code>if</code> statement. When the function returns 0, the condition will be evaluated as true, and when the function returns 1, the condition will be evaluated as false.</p>"},{"location":"developer/utility_functions/#array-utilities","title":"Array utilities","text":"<code>Element_In_Array_Equals_To</code> DescriptionCall example <p>Test if an array contains a given element using string comparison. The function returns 0 if the element is in the array, 1 otherwise.</p> <pre><code>if Element_In_Array_Equals_To 'element' \"${array[@]}\"; then\n    # 'element' is in array\nfi\n</code></pre> <code>Element_In_Array_Matches</code> DescriptionCall example <p>Test if an array contains a given element using regex comparison. The function returns 0 if at least one element in the array matches the regular expression, 1 otherwise.</p> <pre><code>if Element_In_Array_Matches '^file_[0-9]+' \"${array[@]}\"; then\n    # array contains one entry matching the regex\nfi\n</code></pre>"},{"location":"developer/utility_functions/#version-comparison-utilities","title":"Version comparison utilities","text":"<code>Is_Version</code> DescriptionCall example <p>Compare two versions. This function takes two version numbers as first and third arguments and a comparison operator among <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>=</code>, <code>==</code>, <code>!=</code>. Alternatively to the operator, you can use the test comparison options and namely <code>-gt</code>, <code>-lt</code>, <code>-ge</code>, <code>-le</code>, <code>-eq</code> or <code>-ne</code>, respectively. The function returns 0 if the comparison is true, 1 otherwise.</p> <p>Mind redirection</p> <p>If you use this function with a comparison operator containing the <code>&gt;</code> or the <code>&lt;</code> symbol, you need to either quote it or backslash-escape it, otherwise it will be interpreted as redirection.</p> <pre><code>if Is_Version '3.01' \\&gt;= '3.1.0'; then\n    # This if is entered\nfi\nif Is_Version '1.42' '&gt;' '1.42.0.0.1'; then\n    # This if is entered\nfi\nif Is_Version '1.42' -gt '1.42.0.0.1'; then\n    # This if is entered\nfi\n</code></pre>"},{"location":"developer/utility_functions/#variables-and-functions-related-utilities","title":"Variables- and functions-related utilities","text":"<code>Call_Function_If_Existing_Or_Exit</code> DescriptionCall example <p>Check if the given function exists and if so call it forwarding all arguments. Exit with an internal error otherwise.</p> <pre><code># These two are equivalent if the function 'Mystery_Function' is defined\nCall_Function_If_Existing_Or_Exit 'Mystery_Function' 'Arg_1' 'Arg_2' 'Arg_3'\nMystery_Function 'Arg_1' 'Arg_2' 'Arg_3'\n</code></pre> <code>Call_Function_If_Existing_Or_No_Op</code> DescriptionCall example <p>Check if the given function exists and if so call it forwarding all arguments. This is a no-operation if the function is not defined.</p> <pre><code># These two are equivalent if the function 'Mystery_Function' is defined\nCall_Function_If_Existing_Or_No_Op 'Mystery_Function' 'Arg_1' 'Arg_2' 'Arg_3'\nMystery_Function 'Arg_1' 'Arg_2' 'Arg_3'\n</code></pre> <code>Ensure_That_Given_Variables_Are_Set</code> DescriptionCall example <p>Check if the given variables are set, i.e. have been declared. Exit with an internal error if at least one variable is not set.</p> <pre><code>Ensure_That_Given_Variables_Are_Set 'var_1' 'var_2' 'var_3'\n</code></pre> <code>Ensure_That_Given_Variables_Are_Set_And_Not_Empty</code> DescriptionCall example <p>Check if the given variables are set and not empty. Exit with an internal error if at least one variable is not set or set but empty.</p> What does empty mean? <p>A normal variable is empty if it is set to <code>''</code>. An array is considered empty if it has size equal to 0.</p> <pre><code>Ensure_That_Given_Variables_Are_Set_And_Not_Empty 'var_1' 'var_2' 'var_3'\n</code></pre> <code>Make_Functions_Defined_In_This_File_Readonly</code> DescriptionCall example <p>Extract all functions defined in the file where called and mark them as <code>readonly</code>.</p> <p>An important assumption</p> <p>Only functions defined as <code>function Name_Of_The_Function()</code> and the braces on new lines are recognized. Accepted symbols in the function name are letters, <code>_</code>, <code>:</code> and <code>-</code>.</p> <pre><code>Make_Functions_Defined_In_This_File_Readonly\n</code></pre> <code>Print_Not_Implemented_Function_Error</code> DescriptionCall example <p>Print an error about the caller function using the logger.</p> <pre><code>Print_Not_Implemented_Function_Error\n</code></pre>"},{"location":"developer/utility_functions/#yaml-related-utilities","title":"YAML related utilities","text":"<p>All functions have the same interface</p> <p>The following functions need to be called with the YAML string as first argument and the section key(s) as remaining argument(s). As it is assumed everywhere that no key contains a period (or a space), keys can be passed to this function also already concatenated (or in a mixed way).</p> <code>Has_YAML_String_Given_Key</code> DescriptionCall example <p>Test if a YAML string contains a given key. If the YAML string is invalid, an error is printed and the function exits. The function returns 0 if the key is present in the YAML string, 1 otherwise.</p> <pre><code>yaml_string=$'section:\\n  key: 42\\n'\nif Has_YAML_String_Given_Key \"${yaml_string}\" 'section' 'key'; then\n    # this is executed\nfi\n</code></pre> <code>Read_From_YAML_String_Given_Key</code> DescriptionCall example <p>Read a given key from a YAML string. If the YAML string does not contain the key (or it is invalid) the function exits with an error. The read key is printed to standard output.</p> <pre><code>yaml_string=$'section:\\n  key: 42\\n'\nkey_value=$(Read_From_YAML_String_Given_Key \"${yaml_string}\" 'section' 'key')\necho \"${key_value}\"  # &lt;-- this prints '42'\nkey_value=$(Read_From_YAML_String_Given_Key \"${yaml_string}\" 'section.key')\necho \"${key_value}\"  # &lt;-- this prints '42'\n</code></pre> <code>Print_YAML_String_Without_Given_Key</code> DescriptionCall example <p>Remove a given key from a YAML string. If the YAML string does not contain the key (or it is invalid) the function exits with an error. The new YAML string is printed to standard output.</p> <pre><code>yaml_string=$'a: 17\\nb: 42\\n'\nyaml_string=$(Print_YAML_String_Without_Given_Key \"${yaml_string}\" 'b')\necho \"${yaml_string}\"  # &lt;-- this prints 'a: 17'\n</code></pre>"},{"location":"developer/utility_functions/#output-utilities","title":"Output utilities","text":"<code>Print_Line_of_Equals</code> DescriptionCall example <p>Print a line of equal signs to the standard output. Function interface:</p> <ol> <li>Length in characters of the line.</li> <li>Prefix to be printed before the line (optional, default: <code>''</code>).</li> <li>Postfix to be printed after the line (optional, default: <code>'\\n'</code>).</li> </ol> <pre><code>Print_Line_of_Equals 80 '\\e[96m    ' '\\e[0m\\n'\n</code></pre> <code>Print_Centered_Line</code> DescriptionCall example <p>Print a string horizontally centered in the terminal or in the provided length. Function interface:</p> <ol> <li>String to be printed.</li> <li>Length in characters of the line (optional, default: terminal width).</li> <li>Prefix to be printed before the line (optional, default: <code>''</code>).</li> <li>Padding character to fill the line left and right of the string (optional, default: <code>' '</code>).</li> <li>Postfix to be printed after the line (optional, default: <code>'\\n'</code>).</li> </ol> <pre><code>Print_Centered_Line 'Hello world!' 80 '\\e[96m    ' '=' '\\e[0m\\n'\n</code></pre>"},{"location":"developer/utility_functions/#file-utilities","title":"File utilities","text":"<code>Remove_Comments_In_File</code> DescriptionCall example <p>Remove comments starting with a given character (default <code>'#'</code>) in a given file. In particular:</p> <ul> <li>Entire lines starting with a comment (possibly with leading spaces) are removed.</li> <li>Inline comments with any space before them are removed.</li> </ul> <p>Think before using this function!</p> <p>This function considers as comments anything coming after any occurrence of the specified comment character and you should not use it if there might be occurrences of that character that do not start a comment! For the hybrid handler configuration such a basic implementation is enough.</p> <pre><code>Remove_Comments_In_File 'config.yaml'\nRemove_Comments_In_File 'doc.tex' '%'\n</code></pre> Functions to ensure presence or absence of files or folders <p>There are some functions that add a level of abstraction to testing for file or folders existence or absence. Their interface has been unified. In particular, the arguments are interpreted as file or folder names and each is tested. However, if an argument is <code>--</code>, then the arguments before it are interpreted as add-on message to be printed in case of error (one per line).</p> <ul> <li><code>Ensure_Given_Files_Do_Not_Exist</code></li> <li><code>Ensure_Given_Files_Exist</code></li> <li><code>Ensure_Given_Folders_Do_Not_Exist</code></li> <li><code>Ensure_Given_Folders_Exist</code></li> <li><code>Internally_Ensure_Given_Files_Do_Not_Exist</code></li> <li><code>Internally_Ensure_Given_Files_Exist</code></li> </ul> <p>The last ones fail with an internal error instead of a normal fatal one. Note that symbolic links are accepted as arguments and the entity of what they resolve to is tested. Links resolution is done using <code>realpath -m</code> before testing the entity (the option <code>-m</code> accepts non existing paths).</p>"},{"location":"developer/utility_functions/#miscellaneous","title":"Miscellaneous","text":"<code>Strip_ANSI_Color_Codes_From_String</code> DescriptionCall example <p>Remove ANSI color codes from the given string. The cleaned up string is printed to standard output.</p> <pre><code>Strip_ANSI_Color_Codes_From_String $'\\e[96mHi\\e[0m' # &lt;-- this prints 'Hi'\n</code></pre> <code>Print_Option_Specification_Error_And_Exit</code> DescriptionCall example <p>Print a fatal error about the given option using the logger and exits.</p> <pre><code>Print_Option_Specification_Error_And_Exit '--filename'\n</code></pre>"},{"location":"user/","title":"A unique wonderful tool","text":"<p>The hybrid handler is a  Bash script and therefore it does not need any installation. Once cloned the repository, you can simply run the <code>Hybrid-handler</code> script<sup>1</sup>.</p> <ul> <li> <p> Are you ready to go?</p> <p>Software for the desired simulation stages as well as some OS utilities need to be installed.</p> <p>\u00a0 Requirements</p> </li> <li> <p> What's the main idea?</p> <p>Everything can be done using a handy script with different execution modes.</p> <p>\u00a0 The hybrid handler main script</p> </li> <li> <p> Wanna run?</p> <p>Build your configuration file to use the hybrid handler according to your needs.</p> <p>\u00a0 The configuration file</p> </li> <li> <p> Let's try it out!</p> <p>If you want to make a test run or get inspired, predefined setups are available.</p> <p>\u00a0 The predefined configuration files</p> </li> <li> <p> Changing parameters</p> <p>Preparing simulations scanning over one or more parameters is straightforward.</p> <p>\u00a0 Specifying parameters scans</p> </li> <li> <p> Notable changes</p> <p>Check out the CHANGELOG to get insights on the changes between versions.</p> <p>\u00a0 The CHANGELOG</p> </li> <li> <p> Questions?</p> <p>Check out our FAQ section. If needed, feel free to open an issue in the codebase repository.</p> <p>\u00a0 Frequently asked questions</p> </li> <li> <p> Run it in a Docker container</p> <p>Check out our Ubuntu-based Docker image to install software and use the handler.</p> <p>\u00a0 Shipped Docker image</p> </li> </ul> <ol> <li> <p>Be aware, that this might not work straightaway if only a very old Bash installation is available on your OS.   However, if your Bash version is <code>4.x</code> or higher, the main script will be able to give you a complete overview of system requirements.\u00a0\u21a9</p> </li> </ol>"},{"location":"user/configuration_file/","title":"The configuration file","text":"<p>Using YAML syntax it is possible to customize in many different ways which and how different stages of the model are run. The file must be structured in sections (technically these are YAML maps at the top-level). Apart from a generic one, a section corresponding to each stage of the model exists. The presence of any section of this kind implies that the corresponding stage of the model should be run. Many sanity checks are performed at start-up and in case you violate any rule, a descriptive self-explanatory error will be provided (e.g. the order of the stages matters, no stage can be repeated, and so on). If you are new to YAML, be reassured, our YAML usage is definitely basic. Each key has to be followed by a colon and each section content has to be indented in a consistent way. In the following documentation you will find examples, too, and they are probably enough to understand how to create your configuration file.</p>"},{"location":"user/configuration_file/#the-generic-section","title":"The generic section","text":"<p>There is a generic section that contains general information which is not specific to one stage only. This is called <code>Hybrid_handler</code> and it can contain the following key(s).</p> <code>Run_ID</code> <p>This is the name used by the handler to create the folder for the actual run in the stage-dedicated directory. If this key is not specified, a default name containing the date and time of the run is used (<code>Run_YYYY-MM-DD_hhmmss</code>).</p> <p>Note that, if the <code>--id</code> command line option is used when running the handler, this key will be ignored, the value specified on the command line will be used and a message printed to standard output.</p> Example<pre><code>Hybrid_handler:\n    Run_ID: Cool_stuff_1\n</code></pre> <p></p> <code>LHS_scan</code> <p>This key can be provided only in <code>prepare-scan</code> execution mode and its presence enables the Latin Hypercube Sampling algorithm to generate the combinations of parameters values. Its value refers to the number of desired samples and it must be an integer larger than 1. Example<pre><code>Hybrid_handler:\n    LHS_scan: 10\n</code></pre></p>"},{"location":"user/configuration_file/#the-software-sections","title":"The software sections","text":"<p>Each stage of the model has a dedicated section. These are (with the corresponding software to be used):</p> <p> <code>IC</code> for the initial conditions run (SMASH);</p> <p> <code>Hydro</code> for the viscous hydrodynamics stage (vHLLE);</p> <p> <code>Sampler</code> to perform particlization (Hadron sampler) and</p> <p> <code>Afterburner</code> for the last stage (SMASH).</p> <p>As a general comment, whenever a path has to be specified, both an absolute and a relative one are accepted. However, it is strongly encouraged to exclusively use absolute paths as relative ones should be specified w.r.t. different folders (most of the times relatively to the stage output directory).</p> <p>You only need the handler configuration file!</p> <p>Although each software needs a configuration file to be run, you generally do not need to create any. The handler uses a default one if none is explicitly provided. Keys in each software configuration files can be changed from the handler configuration file, without having to create specific configuration files for each software. Refer to the  <code>Software_keys</code> description for further information.</p> <p>Enforced sanity rules</p> <p>Since the hybrid handler understands which software should be run from the presence of the corresponding section, there are a couple of totally natural rules that are enforced and will make the handler fail if violated.</p> <ol> <li>At least one software section must be present in the configuration file.</li> <li>Software sections must be specified in order and without gaps.    This means that it is not possible to e.g. ask the handler to run the initial condition and the sampler stages.</li> </ol>"},{"location":"user/configuration_file/#keys-common-to-all-software-sections","title":"Keys common to all software sections","text":"<code>Executable</code> <p>Path to the executable file of the software to be used. This key is required for all specified stages.</p> <p></p> <code>Config_file</code> <p>Path to the software specific configuration file. If not specified, the file shipped in the configs folder is used.</p> <p></p> <code>Software_keys</code> <p>The value of this key is a YAML map and should be used to change values of the software configuration file. It is not possible to add or remove keys, but only change already existing ones. If you need to add a key to the software default configuration file, you should create a custom one and specify it via the <code>Config_file</code> key. Depending on your needs, you could also create a more complete configuration file and change the values of some keys in your run(s) via this key.  How is this achieved in practice?</p> <p></p> <code>Scan_parameters</code> <p>This key can only be specified in <code>prepare-scan</code> execution mode.</p> <p>List of software input keys whose value is meant to be scanned. Each parameter has to be specified concatenating with a period all the keys as they would appear in the <code>Software_keys</code> map. For example, software keys which read <pre><code>Software_keys:\n  foo:\n    bar: 42\n    baz: 666\n</code></pre> would be specified in the <code>Scan_parameters</code> list as <code>\"foo.bar\"</code> and <code>\"foo.baz\"</code>. Such a list is a YAML array and therefore it can be specified both in the compact and extended form.</p> Compact formExtended form <pre><code>Scan_parameters: [\"foo.bar\", \"foo.baz\"]\n</code></pre> <pre><code>Scan_parameters:\n - \"foo.bar\"\n - \"foo.baz\"\n</code></pre> Each parameter requires a scan specification <p>Parameters specified in the <code>Scan_parameters</code> list need to be accompanied by their scan values to be specified in the <code>Software_keys</code> section of the same stage  the parameters scan syntax.</p>"},{"location":"user/configuration_file/#the-initial-conditions-section","title":"The initial conditions section","text":"<p>There is no specific key of the <code>IC</code> section and only the generic ones can be used.</p> Example<pre><code>IC:\n    Executable: /path/to/smash\n    Config_file: /path/to/IC_config.yaml\n    Software_keys:\n        General:\n            End_Time: 100\n</code></pre>"},{"location":"user/configuration_file/#the-hydrodynamics-section","title":"The hydrodynamics section","text":"<code>Input_file</code> <p>The hydrodynamics simulation needs an additional input file which contains the system's initial conditions. This is the main output of the previous stage and, therefore, if not specified, a  SMASH_IC.dat file is expected to exist in the  IC output sub-folder with the same <code>Run_ID</code>. However, using this key, any file can be specified and used.</p> Example<pre><code>Hydro:\n    Executable: /path/to/vHLLE\n    Config_file: /path/to/vHLLE_config\n    Software_keys:\n        etaS: 0.42\n    Input_file: /path/to/IC_output.dat\n</code></pre>"},{"location":"user/configuration_file/#the-hadron-sampler-section","title":"The hadron sampler section","text":"<p>Also the hadron sampler needs in input the freezeout surface file, which is produced at the previous hydrodynamics stage. However, there is no dedicated <code>Input_file</code> key in the hadron sampler section of the hybrid handler configuration file, because the hadron sampler must receive the path to this file in its own configuration file already. Therefore, the user can set any path to the freezeout surface file by specifying it in the <code>Software_keys</code> subsection, as shown in the example below.</p> <p>By default, if the user does not use a custom configuration file for the hadron sampler and does not specify the path to the freezeout surface file via <code>Software_keys</code>, the hybrid handler will use the configuration file for the hadron sampler which is contained in the  configs folder and in which the path to the freezeout surface is set to <code>=DEFAULT=</code>. This will be internally resolved by the hybrid handler to the path of a  freezeout.dat file in the  Hydro output sub-folder with the same <code>Run_ID</code>,  which is expected to exist. A mechanism like this one is technically needed to be able by default to refer to the same run ID and pick up the correct file from the previous stage. As a side-effect, it is not possible for the user to name the freezeout surface file as <code>=DEFAULT=</code>, which anyways would not probably be a very clever choice. </p> Example<pre><code>Sampler:\n    Executable: /path/to/Hadron-sampler\n    Config_file: /path/to/Hadron-sampler_config\n    Software_keys:\n        surface: /path/to/custom/freezeout.dat\n</code></pre> <p>For the hadron sampler section, there is the additional option to use an alternative sampling software, the FIST sampler. By default, the SMASH-hadron-sampler is used. For using the FIST sampler, the additional input key <code>Module</code> has to be set to <code>FIST</code>. This allows also two separate keys <code>Particle_file</code> and <code>Decay_file</code> to be set, which are the paths to the particle and decay list files respectively, which are shipped with Thermal-FIST. Setting these keys is compulsory. Additionally, using the FIST sampler changes the names of several of the <code>Software_keys</code>. Please refer to the default config for the FIST sampler in  configs.</p> <code>Module</code> <p>The hadron sampler software to be used. At the moment, only the <code>SMASH</code> or <code>FIST</code> values are accepted.</p> <code>Particle_file</code> <p>Path to the particle list file, which is shipped with Thermal-FIST. This key is compulsory when <code>Module</code> is set to <code>FIST</code>.</p> <code>Decays_file</code> <p>Path to the decay list file, which is shipped with Thermal-FIST. This key is compulsory when <code>Module</code> is set to <code>FIST</code>.</p> Example<pre><code>Sampler:\n    Executable: /path/to/FIST-sampler\n    Config_file: /path/to/FIST-sampler_config\n    Module: FIST\n    Particle_file: /path/to/list.dat\n    Decays_file: /path/to/decays.dat\n    Software_keys:\n        hypersurface: /path/to/custom/freezeout.dat\n</code></pre>"},{"location":"user/configuration_file/#the-afterburner-section","title":"The afterburner section","text":"<code>Input_file</code> <p>As other stages, the afterburner run needs an additional input file as well, one which contains the sampled particles list. This is the main output of the previous sampler stage and, therefore, if not specified, a particle_lists.oscar file is expected to exist in the Sampler output sub-folder with the same <code>Run_ID</code>. However, using this key, any file can be specified and used. Note that although it is possible to specify the input for the list modus in SMASH via the <code>Software_keys</code>, this is not allowed here and will result in an error. Always specify the customized input file for the afterburner stage using this key, if needed.</p> <code>Add_spectators_from_IC</code> <p>Whether spectators from the initial conditions stage should be included or not in the afterburner run can be decided via this boolean key. The default value is <code>true</code>.</p> <code>Spectators_source</code> <p>If spectators from the initial conditions stage should be included in the afterburner run, a  SMASH_IC.oscar file is expected to exist in the  IC output sub-folder with the same <code>Run_ID</code>. However, using this key any file path can be specified. This key is ignored if <code>Add_spectators_from_IC</code> is set to <code>false</code>.</p> Example<pre><code>Afterburner:\n    Executable: /path/to/smash\n    Config_file: /path/to/Afterburner_config.yaml\n    Software_keys:\n        General:\n            Delta_Time: 0.25\n    Add_spectators_from_IC: true\n    Spectators_source: /path/to/spectators-file.oscar\n</code></pre>"},{"location":"user/configuration_file/#an-example-of-a-complete-hybrid-handler-configuration-file","title":"An example of a complete hybrid handler configuration file","text":"<p>If you wish to run a simulation of the full model using the default behavior of all the stages of the hybrid handler, then the following configuration file can be used.</p> Example<pre><code>IC:\n    Executable: /path/to/smash\n\nHydro:\n    Executable: /path/to/vHLLE\n\nSampler:\n    Executable: /path/to/Hadron-sampler\n\nAfterburner:\n    Executable: /path/to/smash\n</code></pre> <p>This is going to be costly!</p> <p>Such a configuration file will execute all the modules in production mode, involving a fine hydrodynamic grid and a large statistic of sampled events. It is therefore better suited to be executed on a computer cluster. To test your setup locally, we suggest using the  config_TEST.yaml configuration file  Predefined configuration files.</p> What if I want to omit some stages? <p>Omitting some stages is fine, as long as the omitted one(s) are contiguous from the beginning or from the end. If one or more stages are omitted at the beginning of the model, it is understood that these have been previously run, because the later stages will need input from the previous ones. In such a case, it will be needed to either explicitly provide the needed input file for the first stage in the run or specify the same <code>Run_ID</code> of the simulations already done.</p>"},{"location":"user/docker_image/","title":"An Ubuntu-based framework","text":"<p>Next to the hybrid handler repository, a Docker image based on Ubuntu to compile all SMASH-vHLLE-hybrid software and use the hybrid handler is available. Feel free to download it and use it. In the container all prerequisites of the hybrid handler are satisfied and the user can easily use it. The image does not contain the supported software and its installation is left to the user. However, most libraries are available in the image an a list of them can be obtained inspecting the container. A possible output of docker inspect<pre><code>$ docker inspect \\\n&gt; -f '{{index .Config.Labels \"installed.software.versions\" }}' \\\n&gt; smash-vhlle-hybrid-framework:latest | sed 's/ | /\\n/g'\nDoxygen 1.9.1\nROOT 6.26.10\nHepMC 3.2.5\nRivet 3.1.7\nYoda 1.9.7\nFastjet 3.4.0\nFjcontrib 1.049\nCppcheck 2.8\nCpplint 1.6.0\nyq 4.44.3\n</code></pre></p> <p>The container working directory</p> <p>Running the Docker image, the container will get you in a  /Software directory that is containing the installed software which is needed to e.g. compile SMASH. Please, note that Pythia is missing and it is up to the user to install the needed version of it.</p>"},{"location":"user/execution_modes/","title":"The hybrid handler","text":"<p>Every operation can be done simply by running the  <code>Hybrid-handler</code> executable. This has different execution modes and most of them can be invoked with the <code>--help</code> option to get specific documentation of such a mode.</p> Example about getting help for a given mode<pre><code>./Hybrid-handler do --help\n</code></pre> <p>Each run of the hybrid handler (apart from auxiliary execution modes) makes use of a configuration file and it is the user responsibility to provide one. Few further customizations are possible using command line options, which are documented in the helper of each execution mode.</p> <p>A  inspired user interface</p> <p>If you are used to  Git, you will immediately recognize the analogy. This has been implemented on purpose. Different tasks can be achieved using different execution modes which are analogous to Git commands.</p>"},{"location":"user/execution_modes/#auxiliary-execution-modes","title":"Auxiliary execution modes","text":"Getting helpObtaining the software version <p><pre><code>./Hybrid-handler help\n</code></pre> The <code>help</code> execution mode is the default one. Therefore, if the  <code>Hybrid-handler</code> executable is run without any command line option, an overview of its features is given. This is equivalent to run it with the <code>--help</code> command line option. To be more user friendly, any additional command line option provided in <code>help</code> mode is ignored (even if wrong), and the help message is given.</p> <p><pre><code>./Hybrid-handler version\n</code></pre> The hybrid handler can be asked to print its version. This might be particularly useful to store the handler version as metadata for reproducibility reasons. As many Unix OS tools support a <code>--version</code> command line option, an alias for this mode has been added and the hybrid handler can be run with the <code>--version</code> command line option, too.</p> Why do I just get a warning when asking the version? <p>If for some reason you are not on tagged version of the codebase (e.g. you checked out a different commit), then you will be warned by the hybrid handler that you are not using an official release. This is meant to raise awareness, as it is encouraged to use stable versions only, especially for physics projects.</p>"},{"location":"user/execution_modes/#the-do-execution-mode","title":"The <code>do</code> execution mode","text":"<p>The main <code>do</code> execution mode runs stages of the model and creates a given output tree at the specified output directory (by default this is a subdirectory of the folder from where the handler is run named  data, but it can customized using the <code>--output-directory</code> command line option).</p> <p>Assuming all stages are run, this is the folder tree that the user will obtain.</p> <pre><code>\ud83d\udcc2 Output-directory\n\u2502\n\u251c\u2500 \ud83d\udcc2 IC\n\u2502   \u2514\u2500\u2500\u2500 \ud83d\udcc2 Run_ID\n\u2502          \u2514\u2500 # output files\n\u2502\n\u251c\u2500 \ud83d\udcc2 Hydro\n\u2502   \u2514\u2500\u2500\u2500 \ud83d\udcc2 Run_ID\n\u2502          \u2514\u2500 # output files\n\u2502\n\u251c\u2500 \ud83d\udcc2 Sampler\n\u2502   \u2514\u2500\u2500\u2500 \ud83d\udcc2 Run_ID\n\u2502          \u2514\u2500 # output files\n\u2502\n\u2514\u2500 \ud83d\udcc2 Afterburner\n    \u2514\u2500\u2500\u2500 \ud83d\udcc2 Run_ID\n           \u2514\u2500 # output files\n</code></pre> <p>This might differ depending on how the used handler configuration file has been set up.</p>"},{"location":"user/execution_modes/#the-prepare-scan-execution-mode","title":"The <code>prepare-scan</code> execution mode","text":"<p>This is a different mode, which per se won't run any simulation. Instead, the hybrid handler will prepare future runs by creating configuration files in a sub-folder of the output folder. The user can use the <code>--scan-name</code> command line option to provide a label to the scan, which will name the output sub-folder and will be used as filename prefix. For example, using the default generic <code>scan</code> name, the user will obtain the following tree folder. <pre><code>\ud83d\udcc2 Output-directory\n\u2514\u2500 \ud83d\udcc2 scan\n    \u251c\u2500\u2500\u2500 \ud83d\uddd2\ufe0f scan_combinations.dat\n    \u251c\u2500\u2500\u2500 \ud83d\uddd2\ufe0f scan_run_1.yaml\n    \u251c\u2500\u2500\u2500 \ud83d\uddd2\ufe0f scan_run_2.yaml\n    \u2514\u2500\u2500\u2500 \ud83d\uddd2\ufe0f ...\n</code></pre></p> <ul> <li> <p> The scan parameters must be properly defined in the hybrid handler configuration file using a dedicated syntax.</p> </li> <li> <p> How the parameters combinations are made is described in the documentation of the different types of scans.</p> </li> </ul> <p>Some useful remarks</p> <p>The  <code>scan_combinations.dat</code> file will contain a list of all parameters combinations in a easily parsable table.</p> <p>The  <code>scan_run_*.yaml</code> files are configuration files for the hybrid handler to run physics simulations. For reproducibility reasons as well as to keep track that they belong to a scan, the scan name and parameters are printed in the beginning in commented lines. These files will have leading zeroes in the run index contained in the filename, depending on the total number of prepared simulations. This allows to keep them easily sorted.</p> <p>Good to know</p> <p>If you use the <code>--output-directory</code> in <code>prepare-scan</code> mode to customize the output directory, you will need to specify this command line option again later if you use the <code>do</code> mode and want to store the output of the simulations in the same folder.</p>"},{"location":"user/predefined_configs/","title":"The predefined configuration files","text":""},{"location":"user/predefined_configs/#configuring-the-collision-setups","title":"Configuring the collision setups","text":"<p>There are several complete handler configuration files prepared for the user to run the hybrid handler in its entirety for different collision systems and energies. They follow the setup chosen in  Sch\u00e4fer et al.: Eur.Phys.J.A 58 (2022) 11, 230. However, the finer hydrodynamic grid is by default commented out and has to be uncommented to reproduce the same setup as in the publication. The shear viscosities applied are taken from  Karpenko et al.: Phys.Rev.C 91 (2015) and the longitudinal and transversal smearing parameters are adjusted to improve agreement with experimental data. The supported collision systems are listed in the following table.</p> System \\(\\mathbf{\\sqrt{s_{NN}} \\;\\: \\{GeV\\}}\\) System \\(\\mathbf{\\sqrt{s_{NN}} \\;\\: \\{GeV\\}}\\) Au + Au 4.3 Pb + Pb 6.4 Au + Au 7.7 Pb + Pb 8.8 Au + Au 27.0 Pb + Pb 17.3 Au + Au 39.0 Pb + Pb 2760.0 Au + Au 62.4 Pb + Pb 5020.0 Au + Au 130.0 Au + Au 200.0 <p>They can be found in  configs/predef_configs folder and the user is only required to insert the paths of the executables in the individual software sections. They can be executed in the standard manner using <code>-c</code> option in the execution mode of the handler.</p> Example about running hybrid handler for a predefined setup: Au+Au collision @ 4.3 GeV<pre><code>./Hybrid-handler do -c configs/predef_configs/config_AuAu_4.3.yaml\n</code></pre> <p>Be aware of computational cost!</p> <p>The default predefined setup is meant to be executed on a computing cluster, we do not recommend executing it locally. If you want to test a simple setup on your local machine, refer to the test setup explained here below.</p> <p>Tradeoff between runtime and precision</p> <p>Although SMASH-vHLLE-hybrid conserves in average all charges and almost all energy, this is strongly dependent on the grid in \\(\\eta\\) direction. Therefore, there is a tradeoff between the runtime and memory consumption of the hydrodynamic stage and conservation. The default in the configuration tries to strike a balance, as there is less than 10% loss of energy for central collisions from \\(\\small\\sqrt{s} = 4.3\\;\\mathrm{GeV}\\) to \\(\\small\\sqrt{s} = 200\\;\\mathrm{GeV}\\). The original publication  Sch\u00e4fer et al.: Eur.Phys.J.A 58 (2022) 11, 230 used a considerably finer grid, resulting in around 8 times higher runtime and memory consumption, but also less than 7% loss for small energies and almost perfect conservation for high energies.</p>"},{"location":"user/predefined_configs/#running-a-test-setup","title":"Running a test setup","text":"<p>To test the functionality and to also run it on a local computer, there is the possibility to execute a test setup of the hybrid handler, which is a Au+Au collision at \\(\\small\\sqrt{s} = 7.7\\;\\mathrm{GeV}\\). The statistics are significantly reduced and the grid for the hydrodynamic evolution is characterized by large cells, too large for a realistic simulation scenario.</p> <p>Don't use this setup for production!</p> <p>This test setup is only meant to be used in order to test the functionality of the framework and the embedded scripts, but not to study any realistic physical system.</p> <p>To execute the test, a predefined configuration file is prepared in the same  predef_configs folder. Again, the user needs to insert the paths to the executables in all the software sections.</p> Example about running the test setup of the hybrid handler<pre><code>./Hybrid-handler do -c configs/predef_configs/config_TEST.yaml\n</code></pre>"},{"location":"user/prerequisites/","title":"Hybrid handler prerequisites","text":""},{"location":"user/prerequisites/#simulation-software","title":"Simulation software","text":"<p>Be aware about the meaning of the version requirements</p> <p>In the following we state a version requirement for the external software needed in the various phases. Strictly speaking, this is not a requirement for the hybrid handler, which most likely will correctly work even if different versions of the physics software are used. However, the hybrid handler makes use of some default configuration files for each software and this does rely on the version of the given software. Said differently, if you e.g. need to use older versions of some software, expect to have to specify a different base configuration for that given software  configuration keys documentation.</p> Physics Software Suggested version SMASH 3.1 or higher<sup>1</sup> vHLLE Tag <code>vhlle-smash-hybrid-1</code> vHLLE parameters Tag <code>vhlle-smash-hybrid-1</code> Hadron sampler See below Other software Required version Python 3.2 or higher <p>Instructions on how to compile or install the software above can be found at the provided links either in the official documentation or in the corresponding README files.</p> <p>Be consistent about dependencies</p> <p>The above prerequisites have in general additional dependencies and it is important to be consistent in compiler options when compiling these and the software itself. We particularly highlight that the newer versions of ROOT require C++17 bindings or higher, which calls for proper treatment of compiler options in SMASH and the hadron sampler. It is also recommended to start from a clean build directory whenever changing the compiler or linking to external libraries that were compiled with different compiler flags.</p> <p>In principle, the Hybrid handler is agnostic to the physics model used in each state, and is built in a way to support different software with minimal efforts. Currently, this is realized for some of the different stages and the supported software is reported here below.</p>"},{"location":"user/prerequisites/#hadron-sampler","title":"Hadron Sampler","text":"Supported Software Required version Hadron sampler Same as SMASH<sup>2</sup> FIST sampler Commit <code>af99229</code> or later"},{"location":"user/prerequisites/#unix-system-requirements","title":"Unix system requirements","text":"<p>The hybrid handler makes use of many tools which are usually installed on Unix systems. For some of them a minimum version is required and for the others their availability is enough. However, in some cases, the GNU version is required and on some Linux distribution or on Apple machines the default installation might not be suitable. To check out what is required and what is available on your system, simply run the <code>Hybrid-handler</code> executable without options: An overview of the main functionality as well as a system requirements overview will be produced.</p> <p>Use <code>brew</code> to install needed GNU utilities on Apple machines</p> <p>Often macOS is shipped with the BSD implementation of tools like <code>awk</code>, <code>sed</code>, <code>wc</code>, <code>sort</code> and many others. Since the GNU version of some tools offers more functionality, it has been decided in few cases to prefer these, especially since most supercomputers in the scientific field have such a version installed by default. However, on Apple machines, the <code>brew</code> package manager can be easily used to install the possibly missing utilities.</p> <p>Please, note that the needed commands must be available and automatically findable by your bash shell. For example, installing GNU AWK via <code>brew install gawk</code> is not enough as, by default, it only provides the <code>gawk</code> command and the hybrid handler needs <code>awk</code> instead. You then need to adjust the <code>PATH</code> environment variable as suggested by <code>brew</code> itself: <pre><code>export PATH=\"${HOMEBREW_PREFIX}/opt/gawk/libexec/gnubin:${PATH}\"\n</code></pre> Unfortunately, there is no standard way to figure out which implementation a command offers. However, all GNU commands support the <code>--version</code> command line option and their output contains the <code>GNU</code> word. This allows to understand if the needed GNU version is available or if the commands refers to something else.</p>"},{"location":"user/prerequisites/#python-requirements","title":"Python requirements","text":"<p>You need the Python <code>packaging</code> module installed!</p> <p>The handler uses Python itself to check Python requirements and it needs to use the <code>packaging</code> module to do so. Make sure to have it available before starting, otherwise the handler will produce a non-fatal error mentioning this aspect.</p> <p>Few standalone Python scripts are used for dedicated tasks and this implies that the hybrid handler will terminate with an error if some of these requirements are missing. However, since not all requirements are always needed, the hybrid handler will only check for some of them on a per-run basis. In the system overview obtained by running the <code>Hybrid-handler</code> executable without options, also Python requirements are listed, each with a short description about when such a requirement is needed.</p> <p>I simply want to install all requirements. What should I do?</p> <p>In the  python folder, you'll find a  requirements.txt file which you can use to set up a dedicated Python virtual environment. Alternatively, although discouraged, you can simply run (from the repository top-level) <pre><code>pip install --user -r python/requirements.txt\n</code></pre> to install the requirements globally for your user.</p> <ol> <li> <p>Version <code>3.1</code> is only needed for the afterburner functionality. Otherwise version <code>1.8</code> is sufficient.\u00a0\u21a9</p> </li> <li> <p>As SMASH is a dependency of the hadron sampler codebase, different versions of the latter have to be compiled with corresponding given versions of SMASH. However, if you need to use a different version of the hadron sampler software, this is probably not making a difference from the Hybrid handler perspective and it is likely to work.\u00a0\u21a9</p> </li> </ol>"},{"location":"user/scans_syntax/","title":"The parameters scan syntax","text":"<p>In order to properly run the hybrid handler in <code>prepare-scan</code> mode, the configuration file must be properly created and fulfil few additional constraints.</p> <p>Do not forget to declare scan parameters as such!</p> <p>Each parameter which must be scanned has to be declared so by using the <code>Scan_parameters</code> key in the corresponding stage section  key description. If a parameter is not declared to be scanned and is specified as a scan in the <code>Software_keys</code> section, this will probably not be caught by the hybrid handler and the produced configurations are likely to be wrong.</p> <p>Only scanning numerical parameters is possible</p> <p>At the moment, it is not possible to scan parameters whose value is not numerical. More precisely, only integer, float or boolean YAML types are accepted. Feel free to open an issue if this is a too strong restriction for you.</p> <p>Once scan parameters have been specified as such, they must appear in the <code>Software_keys</code> map. However, their value should not be a simple parameter value, but a YAML map with a given format. In the following we will refer to this map as \"scan object\". The different allowed ways to specify scan objects are discussed in the following, providing an example for each of them. The scan object shall always have a <code>Scan</code> key as single top-level key. Generic parameter scan specification<pre><code>Scan_parameters: [\"Parameter\"]\nSoftware_keys:\n  Parameter:\n    Scan:\n      ...\n</code></pre></p>"},{"location":"user/scans_syntax/#explicitly-specifying-the-parameter-values","title":"Explicitly specifying the parameter values","text":"<p>The most basic way to specify a scan is by providing the list of its values. This is possible in the <code>Values</code> YAML array inside the <code>Scan</code> map.</p> Compact styleMixed styleExtended style Example<pre><code>Scan_parameters: [\"foo.bar\"]\nSoftware_keys:\n  foo:\n    bar: {Scan: {Values: [17, 42, 666]}}\n</code></pre> Example<pre><code>Scan_parameters: [\"foo.bar\"]\nSoftware_keys:\n  foo:\n    bar:\n      Scan:\n        Values: [17, 42, 666]\n</code></pre> Example<pre><code>Scan_parameters: [\"foo.bar\"]\nSoftware_keys:\n  foo:\n    bar:\n      Scan:\n        Values:\n         - 17\n         - 42\n         - 666\n</code></pre>"},{"location":"user/scans_syntax/#latin-hypercube-sampling","title":"Latin Hypercube Sampling","text":"<p>This type of scan has to be explicitly enabled by using the <code>LHS_scan</code> key. Once done so, for each scan parameter, a range in which the values will be sampled has to be specified using the <code>Range</code> YAML array inside the <code>Scan</code> map.</p> Compact styleMixed styleExtended style Example<pre><code>Scan_parameters: [\"foo.bar\"]\nSoftware_keys:\n  foo:\n    bar: {Scan: {Range: [-17, 666]}}\n</code></pre> Example<pre><code>Scan_parameters: [\"foo.bar\"]\nSoftware_keys:\n  foo:\n    bar:\n      Scan:\n        Range: [-17, 666]\n</code></pre> Example<pre><code>Scan_parameters: [\"foo.bar\"]\nSoftware_keys:\n  foo:\n    bar:\n      Scan:\n        Range:\n         - 17\n         - 666\n</code></pre>"},{"location":"user/scans_types/","title":"Types of scans","text":"<p>If a parameter scan is created with more than one scan parameter, it has to be decided how the different values for each parameter will be combined.</p>"},{"location":"user/scans_types/#all-combinations-by-default","title":"All combinations by default","text":"<p>Unless differently specified, all combinations of parameters values are considered and one output handler configuration file per combination will be created.</p> <p>From the mathematical point of view, given \\(n\\) scan parameters with set of values \\(X_1, ..., X_n\\,\\), the set of considered combinations is nothing but the \\(n\\)-ary Cartesian product over all sets of values,</p> \\[ X_1 \\times \\dots \\times X_n = \\bigl\\{(x_1, ..., x_n) \\;|\\; x_i \\in X_i \\quad\\forall\\, i \\in \\{1,...,n\\} \\bigr\\} \\] <p>For example, specifying two different scan parameters with 2 and 5 values, respectively, 10 values combinations will be built and 10 output files produced.</p> Handler configuration Scan folder Scan combinations <pre><code># ...\nScan_parameters: [\"Foo.Bar\", \"Foo.Baz\"]\nSoftware_keys:\n  Foo:\n    Bar: {Scan: {Values: [-1, 0, 17, 42, 666]}}\n    Baz: {Scan: {Values: [True, False]}}\n# ...\n</code></pre> <pre><code>$ ls scan\nscan_combinations.dat    scan_run_04.yaml    scan_run_08.yaml\nscan_run_01.yaml         scan_run_05.yaml    scan_run_09.yaml\nscan_run_02.yaml         scan_run_06.yaml    scan_run_10.yaml\nscan_run_03.yaml         scan_run_07.yaml\n</code></pre> <pre><code># Parameter_1: Foo.Bar\n# Parameter_2: Foo.Baz\n#\n#___Run  Parameter_1  Parameter_2\n      1           -1         True\n      2           -1        False\n      3            0         True\n      4            0        False\n      5           17         True\n      6           17        False\n      7           42         True\n      8           42        False\n      9          666         True\n     10          666        False\n</code></pre> What happens if I provide a single value for a scan parameter? <p>If you provide a single-value list to <code>Values</code>, this will be accepted by the hybrid handler and the provided value will be considered in all combinations. If this happen to be the only provided scan parameter, a single configuration file will be created together with a basically useless single-combination file. </p>"},{"location":"user/scans_types/#latin-hypercube-sampling","title":"Latin Hypercube Sampling","text":"<p>This algorithm, if enabled, samples multidimensional parameters randomly, while keeping the distance between samples maximal, and is commonly used for Bayesian inference. Refer to e.g.  this page for more information. The sampling itself is done by calling the <code>lhs</code> function from the  pyDoe Python library function, using the <code>centermaximin</code> criterion.</p>"},{"location":"user/CHANGELOG/","title":"Hybrid handler CHANGELOG","text":"<p>All notable changes to this project will be documented in this changelog. This project does not strictly adhere to Semantic Versioning, but it uses versioning inspired by it. In particular, not all backward incompatible changes lead to a bump in the major version number, but all of these are mentioned and emphasized here. Given a version number <code>X.Y.Z</code>,</p> <ul> <li><code>X</code> is incremented for major changes in particular relevant new functionality,</li> <li><code>Y</code> is incremented for minor changes or new minor functionality, and</li> <li><code>Z</code> is mainly used for bug fixes.</li> </ul> Symbols <p>Every entry in this file is prepended with a symbol that is meant to draw attention about the type of change. Click on a symbol above for more information about it.</p> <p>This symbol indicates new features.</p> <p>This symbol indicates changes in existing functionality.</p> <p>This symbol indicates fixes of wrong behavior.</p> <p>This symbol indicates removed features.</p> <p>This symbol indicates breaking changes, i.e. not backward-compatible changes.</p> <p>This symbol indicates deprecated features, which are likely to be removed in later versions.</p> <p>This symbol indicates changes that deserve particular attention by the user.</p> <p>Unreleased</p> <p>Changes:</p>"},{"location":"user/CHANGELOG/#smash-vhlle-hybrid-212","title":"SMASH-vHLLE-hybrid-2.1.2","text":"Release date: 2025-05-16 \u2003  Compare changes to previous version <p> \u00a0 The previous hot-fix introduced a subtle bug, making the hybrid handler ignore a user-customized base configuration file for the <code>IC</code> stage. This is fixed now.</p>"},{"location":"user/CHANGELOG/#smash-vhlle-hybrid-211","title":"SMASH-vHLLE-hybrid-2.1.1","text":"Release date: 2025-05-15 \u2003  Compare changes to previous version <p> \u00a0 Make the handler select the correct default base configuration file for the <code>IC</code> stage depending on the SMASH version. This was needed because <code>SMASH-3.2</code> changed some configuration keys about initial conditions setup.</p> <p> \u00a0 Fix how spectators are added from the <code>IC</code> output into the <code>Afterburner</code> input file. This now works for all SMASH versions. The spectators from the target were not properly considered beforehand. Additionally, the adding of spectators is currently only allowed if only one <code>IC</code> event was run.</p> <p> \u00a0 Renamed the copied/linked afterburner inputfile containing the sampled particles (and possibly spectators) from  sampled_particles_list.oscar to :material_file: sampled_particles.oscar. Note that this is not a breaking change because this file is created into the  Afterburner folder at the beginning of such a stage.</p>"},{"location":"user/CHANGELOG/#smash-vhlle-hybrid-21","title":"SMASH-vHLLE-hybrid-2.1","text":"Release date: 2025-03-31 \u2003  Compare changes to previous version <p> \u00a0 Make the handler select the correct default base configuration file for the <code>Sampler</code> stage depending on the SMASH hadron sampler version. This was needed because <code>SMASH-hadron-sampler-3.2</code> refactored the user interface.</p> <p> \u00a0 Support for a new sampler module: FIST sampler is now usable additionally to the SMASH hadron sampler.</p> <p> \u00a0 The project logo was changed to a fancier, more fantasy version.</p>"},{"location":"user/CHANGELOG/#smash-vhlle-hybrid-20","title":"SMASH-vHLLE-hybrid-2.0","text":"Release date: 2024-04-12 \u2003  Compare changes to previous version <p>  \u00a0 The project approach has totally changed and it switched from a <code>CMake</code> based framework to a <code>Bash</code> based framework.   A detailed documentation web-page has been built to guide both users and developers.</p>"},{"location":"user/CHANGELOG/#smash-vhlle-hybrid-10","title":"SMASH-vHLLE-hybrid-1.0","text":"<p> \u00a0 Release date: 2020-11-18 \u2003  First public version of the SMASH-vHLLE-hybrid</p>"},{"location":"user/FAQ/","title":"Frequently asked questions","text":""},{"location":"user/FAQ/#how-can-i-add-a-software-key-to-a-configuration-file","title":"How can I add a software key to a configuration file?","text":"<p>Using the hybrid handler, many different programs are run and each of them needs a configuration file. The hybrid handler also needs a configuration file and having many of these might lead to some confusion.</p> <p>All the configuration files of the separate software for each state can be specified by the user, but do not have to. The hybrid handler is using default ones, which are shipped in the  configs folder. You can have a look to these files to see which keys are used there.</p> <p>Since using the <code>Software_keys</code> key in the hybrid handler configuration file it is only possible to change the value of an existing input key in the software configuration file, it would result in an error to attempt to add a missing one. Analogously, there is no mechanism to remove keys from the default base software configuration file used by the handler.</p> <p>Let's see in a concrete example how to proceed if different keys are needed. Assume we want to run the  afterburner stage asking SMASH not to use the grid for interaction lookup (this might be of course any other key). The following setup would not work, as the <code>Use_Grid</code> key in the <code>General</code> section is not provided in the default base configuration file for the afterburner.</p> Hybrid-handler configuration file SMASH Afterburner base configuration file <pre><code>Afterburner:\n    Executable: \"/path/to/smash\"\n    Software_keys:\n        General:\n            End_Time: 500\n            Nevents: 200\n            Use_Grid: False # (1)!\n</code></pre> <ol> <li> This triggers an error as it is missing in the afterburner base configuration file!</li> </ol> <pre><code>Logging:\n    default: INFO\n\nGeneral:\n    Modus:         List\n    Time_Step_Mode: None\n    Delta_Time:    0.1\n    End_Time:      10000.0\n    Randomseed:    -1\n    Nevents:       1000\n\nCollision_Term:\n    Strings: True\n    String_Parameters:\n        Use_Monash_Tune: False\n\nOutput:\n    Particles:\n        Format:          [\"Oscar2013\"]\n\nModi:\n    List:\n        File_Directory: \".\"\n        Filename: \"sampled_particles.oscar\"\n</code></pre> <p>The correct way to fix the problem is to create a new SMASH afterburner configuration file, having all the keys that need to be modified and then modify them from the hybrid handler configuration file after having specified that a custom configuration file should be used for the afterburner stage.</p> Hybrid-handler configuration file My own SMASH Afterburner base configuration file <pre><code>Afterburner:\n    Executable: \"/path/to/smash\"\n    Config_file: \"/path/to/my/base-config.yaml\"\n    Software_keys:\n        General:\n            End_Time: 500\n            Nevents: 200\n            Use_Grid: False # (1)!\n</code></pre> <ol> <li> Now this is working, as the key exists in the base configuration file.</li> </ol> <pre><code>Logging:\n    default: INFO\n\nGeneral:\n    Modus:         List\n    Time_Step_Mode: None\n    Delta_Time:    0.1\n    End_Time:      10000.0\n    Randomseed:    -1\n    Nevents:       1000\n    Use_Grid:      True\n\n# [...] &lt;- SAME AS BEFORE!\n</code></pre> <p>A possible way to go in real life</p> <p>In complex projects it might be handy to prepare base configuration files for the needed stages in the beginning. A good idea might be to keep them close to the data in a known, dedicated folder. This ensures reproducibility at any point during the project and can be seen as a good data management practice. Of course, you will need to point to your custom base configuration files from within the hybrid handler configuration file in the different stages blocks.</p> <pre><code>\ud83d\udcc2 Project-directory\n\u2502\n\u251c\u2500 \ud83d\udcc2 Custom_base_configuration_files\n\u2502   \u251c\u2500\u2500\u2500 \ud83d\uddd2\ufe0f smash_IC.yaml\n\u2502   \u251c\u2500\u2500\u2500 \ud83d\uddd2\ufe0f vhlle.txt\n\u2502   \u2514\u2500\u2500\u2500 ...\n\u251c\u2500 \ud83d\udcc2 IC\n\u2502   \u2514\u2500\u2500\u2500 ...\n\u251c\u2500 \ud83d\udcc2 Hydro\n\u2502   \u2514\u2500\u2500\u2500 ...\n\u2514\u2500\u2500\u2500 ...\n</code></pre> <p>Do not edit the shipped base configuration file!</p> <p>Although this might be a quick way to try out something, you should never prefer to change the base configuration files in the hybrid handler repository. Your changes might get in conflict with future releases of the software or simply be undone by accident using Git inside the repository.</p>"},{"location":"user/FAQ/#can-i-collide-particles-other-than-nuclei-in-the-ic-stage","title":"Can I collide particles other than nuclei in the IC stage?","text":"<p>You might have tried to use SMASH as initial-conditions software specifying a pion as the projectile via the following configuration file, but this was not accepted by the hybrid handler.</p> Hybrid-handler configuration file<pre><code>IC:\n    Executable: \"/path/to/smash\"\n    Software_keys:\n        Modi:\n            Collider:\n                Projectile:\n                    Particles: {211: 1} # (1)!\n</code></pre> <ol> <li> This triggers an error as <code>Particles</code> is a YAML map and it contains a sub-map.     Since <code>211</code> is a map key, which is not present in the base configuration file, this line triggers an error.</li> </ol> <p>The SMASH  <code>IC</code> base configuration file is set up to collide nuclei and, therefore, both the projectile and the target are made of protons and neutrons. These are specified using PDG codes as YAML map keys in the configuration file. Any attempt to use different particles is a try to use a new key in the configuration file, namely one not present in the base configuration file! If you need to collide particles other than protons and nucleons, you need your own base configuration file. Check out this other question for detailed instructions.</p>"},{"location":"user/FAQ/#how-can-i-repeat-the-same-run-in-parallel-to-increase-statistics","title":"How can I repeat the same run in parallel to increase statistics?","text":"<p>The output folder in <code>do</code> mode is built by the hybrid handler in a way such that runs with different IDs do not interfere. Therefore, the same run can be repeated as many times as desired, on constraint that the user specifies a different run ID for each run. By default, the run ID contain a time stamp and, therefore, waiting at least one second before starting different instances of the same run and using the default run ID should achieve what desired.</p> <p>The run ID can also be specified on the on the command line via the <code>--id</code> option. This can be exploited to easily start the same run several times with different run IDs and naming these as desired, without necessarily relying on the default timestamp.</p> Using the default run IDSpecifying the run ID <pre><code>cd Project_folder\nfor ((index = 0; index &lt; ${SLURM_CPUS_ON_NODE}; index++ )); do # (1)!\n    Hybrid_handler do -c experiment.yaml &amp;\n    sleep 1\ndone\nwait\n</code></pre> <ol> <li>The <code>SLURM_CPUS_ON_NODE</code> variable is peculiar to slurm scheduler.     This <code>for</code> loop should submit as many runs as the number of CPUs requested by the job.     Clearly, it is only a proof of concept and adjustment might be needed.</li> </ol> <pre><code>cd Project_folder\nfor ((index = 0; index &lt; ${SLURM_CPUS_ON_NODE}; index++ )); do\n    Hybrid_handler do \\\n        -c experiment.yaml \\\n        --id \"Run_$(printf \"%0d\" ${index})\" &amp;\ndone\nwait\n</code></pre>"}]}